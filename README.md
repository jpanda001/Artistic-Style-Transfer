# Artistic Style Transfer

The objective of this project is to design a deep learning model that manipulates photographic
images to adopt the visual style and colouring patterns of artistic images while preserving the
original shape and geometry of the photographic images. Figure 1 below shows an example
of this image manipulation process which we referred to as artistic style transfer.

<p align="center", class="is-small-text">
  <img src="https://github.com/jpanda001/Artistic-Style-Transfer/blob/master/Document_Images/model_architecture.PNG">

*Figure 1: Visualization of a style transfer process. The transformed image preserves the shape and geometry of
the content image, but adopts the style and colouring pattern of the style images. **(In this report, all
photographic images are called content images while all artistic images are called style images)***
</p>

In this project, we use artistic images from three different style classes: pencil sketch,
watercolour paintings and oil paintings. Furthermore, we train three different versions of our
model, one for each style class.

Artistic Style Transfer is very useful for the entertainment industry. It could help create new
artworks, and make it much easier and quicker to edit and style images by minimizing the
need to manually photoshop images. Furthermore, artistic style transfer could also be used as
a data augmentation technique to increase the size and quality of a training dataset.

The architecture of our project is shown in Figure 2 below.

<p align="center", class="is-small-text">
  <img src="https://github.com/jpanda001/Artistic-Style-Transfer/blob/master/Document_Images/Model_prototype.PNG">

*Figure 2: Visualization of the architecture used for our project*
</p>

## Data Collection & Processing

For our project, content images were collected from ImageNet and style images were
collected from Behance Artistic Media Dataset (BAMD). A total of 5000 content images
were collected from ImageNet. A total of 5000 style images from each of BAMD’s
pencil-sketch, oil-painting and watercolour-painting style classes were collected for the style
image dataset. The content images were split into 75% training, 15% validation and 15%
testing datasets, whereas all of the style images were used only for training the neural
network.

All data-collection and data-cleaning processes were conducted with the help of a python
script. List of image URLs were extracted from database files and stored in panda dataframes.
Then, images were downloaded from the URLs using `urllib.request.urlretrieve(path)`

Following image cleaning operations were undertaken before saving the
images on a google drive dataset folder:
1.  Images were center-cropped to dimensions of 256 x 256 if their initial dimensions
    were larger. If the initial dimensions were smaller, the images were upscaled with
    padding to 256 x 256 so that their initial aspect ratios were preserved. These
    operations were conducted using Numpy and Image modules in python. Figure 2
    shows a sample of cleaned content image, style image and an upscaled content image
    with border paddings.

2.  It was ensured that all images had exactly 3 color channels (i.e. any grayscale images
    or images with 1 color dimension were eliminated).

<p align="center", class="is-small-text">
  <img src="https://github.com/jpanda001/Artistic-Style-Transfer/blob/master/Document_Images/Image_padding.PNG">

*Figure 3: A cleaned sample image of (a) upscaled content image with padding, (b) a center-cropped content
image and (c) a center-cropped style image*
</p>

## Architecture

Our artistic style transfer model is based on a generative adversarial network (GAN)
architecture as shown in Figure 2. The image transformation network of the GAN transforms
content images so that they adopt the style of style images. Discriminator of the GAN is a
pre-trained VGG-19 network which generates the content and style representations of the
content, style and transformed images. The content representation is obtained from the
feature maps of the 4th convolutional layer of the discriminator, and the style representation
is obtained by taking the linear combination of feature maps from the 1st, 2nd 3rd and 4th
convolutional layer of the discriminator. Content loss of our model is generated by comparing
the content representation of the content image and transformed image, and style loss is
generated by comparing the style representations of the style image and transformed image.
Total loss of our model is calculated by taking the scalar multiple of content and style loss.
Image-transformation network is trained through backward propagation of this total loss.

Our current image transformation network has an encoder and a decoder block, with 5
ResNet blocks connecting the two. The encoder block has 4 convolutional layers, each
followed by a 2D instance-normalization layer. The decoder block has 4
convolution-transpose layers, each also followed by a 2D instance-normalization layer. There
are five ResNet blocks between the encoder and decoder blocks, each with 2 convolutional
layers. Figure 4 provides a visual representation of this architecture.

<p align="center", class="is-small-text">
  <img src="https://github.com/jpanda001/Artistic-Style-Transfer/blob/master/Document_Images/Transformation_prototype.PNG">

*Figure 4: The architecture of our image transformation network developed based on Keiji Yanai’s Conditional
Fast Style Transfer Network [3].*
</p>

## Baseline Model

The baseline model uses a GAN architecture similar to the main style transfer model above,
but uses an image optimization method instead of implementing an image transformation
network. In optimization-based method, the pixel values of the transformed image are
optimized directly during the backward pass of a training step, instead of optimizing the
parameters of the image transformation network.

While running our baseline model, the content image is selected as the initial transformed
image. Then, the pixel values of the transformed image are gradually optimized over multiple
iterations so that the image adopts the style of the style image while retaining the content of
the content image.

Since this model uses no neural networks to learn how the style transfer process works in
general, it has to be trained for multiple iterations for every new pair of content and style
image. On the other hand, our working-model can generate a transformed image with a single
forward pass after it has been trained adequately.

## Results

### Quantitative

We trained three versions of the working-model, one for each style class using 3500 content
images and 4 style images. We evaluate the performance of our working-models by
comparing it to the performance of our baseline model based on two parameters: average test
loss and average test runtime. After training the working model for 40000 iterations, we got
the training loss curve as shown in Figure 5. We computed the average test loss across all
three versions of our working-models to be 59.36, and the average test runtime to be around
0.5s. On the other hand, the average loss and the average runtime for the baseline model
across the three style classes were computed to 32.25 and around 600s respectively. Based on
these results, we can say that the baseline model optimizes loss better than the working
model, while taking significantly more time to generate a stylized image.

### Qualitative

The three pictures below show the test result obtained for each style class from the working
model and the baseline model. Following observations can be made for these results:
* The baseline images have more colours and details compared to the working-model
image. This is because the baseline model trains for every new content image before
transforming it. So, the content and style features are much better organized in the
baseline image than in the working-model image.
* The working-model images for pencil-sketch and watercolour style classes have lines
and uneven colouring near the borders. This could be because some style images in
the aforementioned style classes had lines and strips of a single colour near their
image borders. The working-model could have interpreted these as important style
features and transferred them to the content images.
* All the baseline images have strange lines in their top left region. This could be
removed by running the baseline model for more iterations.

<p align="center", class="is-small-text">
  <img src="https://github.com/jpanda001/Artistic-Style-Transfer/blob/master/Document_Images/Result_pencil.PNG">

*Figure 5: Baseline Result (left) vs. working-model result (right) using pencil sketch as style image*
</p>

<p align="center", class="is-small-text">
  <img src="https://github.com/jpanda001/Artistic-Style-Transfer/blob/master/Document_Images/Result_watercolour.PNG">

*Figure 6: Baseline Result (left) vs. working-model result (right) using watercolor painting as style image*
</p>

<p align="center", class="is-small-text">
  <img src="https://github.com/jpanda001/Artistic-Style-Transfer/blob/master/Document_Images/Result_oil.PNG">

*Figure 7: Baseline Result (left) vs. working-model result (right) using oil painting as style image*
</p>

### Discussion

Despite the fact that our working-model generates images of lesser quality and computes a higher test loss than the baseline model, we still consider it to have a better performance than the baseline model for several reasons. First of all, the baseline model takes numerous steps of training to transform every new content image, but the working-model transforms any new content image with a single forward pass (given it has been trained once before). So, the working-model is much quicker to run than the baseline model. Furthermore, the working-model learns a general method to stylize any given content image unlike the baseline model which has to train and learn for every new content image. This means that the working-model has a better understanding of the image stylization process than the baseline model.  
 
There are still some issues with our working-model. As mentioned before, our working-model does inadequate style transformation near the borders of the transformed images. This can be fixed by removing style images with strange border features or by making minor changes to the architecture of our model. 

## Ethical Considerations

There are several ethical issues to consider about style transfer models. First of all, these models might replace numerous jobs in the entertainment industry, especially photo/video editing and stylization jobs. Secondly, there could be conflicts about the ownership of artworks generated by style transfer models. For example, if an image is transformed using a unique style of an artist, will the ownership of the transformed image go to the artist or the creator of the style transfer model? Lastly, style transfer models could generate artworks that are as qualitative as artworks created by highly-skilled artists. So, people might lose appreciation for the hard-work and creativity that goes into making art. 
