# -*- coding: utf-8 -*-
"""Main_Stylization_Program.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t_uNtq7ZmMw3ZS7uVRvyewQzM1MJO8rc
"""

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import torchvision
import torchvision.models as models
import nonechucks as nc
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import copy
import matplotlib.pyplot as plt
import random
from PIL import Image
vgg = models.vgg19(pretrained=True).features.cuda() #only the features portion
for parameter in vgg.parameters(): #Fix vgg network parameters
  parameter.requires_grad_(False)

from google.colab import drive
drive.mount('/content/gdrive')

#LOAD STYLE AND CONTENT IMAGES AND NORMALIZE THEM TO NORMALIZED TENSORS
def image_set(set_path, img_size=256):      
    transform = transforms.Compose([
                        transforms.CenterCrop(256),
                        transforms.ToTensor(),
                        transforms.Normalize((0.485, 0.456, 0.406), 
                                             (0.229, 0.224, 0.225))])

    dataset = torchvision.datasets.ImageFolder(root = set_path, transform = transform)
    print(dataset)
    return dataset

content_set = image_set(set_path='/content/gdrive/My Drive/APS360_Style_Transfer/Main_images/other imgs', img_size=256)
graphite_set = image_set(set_path='/content/gdrive/My Drive/APS360_Style_Transfer/Main_images/Style_graphite', img_size=256)
oilpaint_set = image_set(set_path='/content/gdrive/My Drive/APS360_Style_Transfer/Main_images/Style_oilpaint', img_size=256)
pen_ink_set = image_set(set_path='/content/gdrive/My Drive/APS360_Style_Transfer/Main_images/Style_pen_ink', img_size=256)
watercolour_set = image_set(set_path='/content/gdrive/My Drive/APS360_Style_Transfer/Main_images/Style_watercolour', img_size=256)

def get_style_rep(image):
  layer = {'0': 'conv1_1',
           '5': 'conv2_1', 
           '10': 'conv3_1', 
           '19': 'conv4_1',
           '28': 'conv5_1'}
        
  rep = {}
  i = image.cuda()                           
  for index, name in vgg._modules.items(): #holds modules in the model
      i = name(i)
      if index in layer:
        rep[layer[index]] = i           
  return rep

def get_content_rep(image):
  layer = {'21': 'conv4_2',  # content representation layer as suggested by the paper
           }
        
  rep = {}
  i = image.cuda()                           
  for index, name in vgg._modules.items(): #holds modules in the model
      i = name(i)
      if index in layer:
        rep[layer[index]] = i           
  return rep

def get_target_rep(image):
  layer = {'0': 'conv1_1',
           '5': 'conv2_1', 
           '10': 'conv3_1', 
           '19': 'conv4_1',
           '21': 'conv4_2',
           '28': 'conv5_1'}
        
  rep = {}
  i = image.cuda()                           
  for index, name in vgg._modules.items(): #holds modules in the model
      i = name(i)
      if index in layer:
        rep[layer[index]] = i           
  return rep

def gram_matrix(tensor):
    # NCWH of the input
    n, c, h, w = tensor.size()
    #resize
    features = tensor.view(n * c, h * w) 
    # Gram product
    G = torch.mm(features, features.t())
    #normalize product
    gm = G.div(n * c * h * w)
    
    return gm

def get_content_loss(content, target):
  loss = F.mse_loss(content, target)
  return loss

def get_style_loss(style_g, target_g):
  loss = F.mse_loss(style_g,target_g)
  return loss

"""# Transformation Network
The transformtion network architecture is coded here as per Keiji Yanai's 
Fast Style Transfer Network.
"""

class TransformerNet(torch.nn.Module):
    def __init__(self):
        super(TransformerNet, self).__init__()
        # Initial convolution layers
        self.conv1 = nn.Conv2d(3, 32, kernel_size=9, stride=1, padding=4, padding_mode='reflect')
        self.in1 = torch.nn.InstanceNorm2d(32, affine=True) 
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, padding_mode='reflect')
        self.in2 = torch.nn.InstanceNorm2d(64, affine=True) 
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, padding_mode='reflect')
        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)
        # Residual layers
        self.res1 = ResidualBlock(128)
        self.res2 = ResidualBlock(128)
        self.res3 = ResidualBlock(128)
        self.res4 = ResidualBlock(128)
        self.res5 = ResidualBlock(128)
        # Upsampling Layers
        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)
        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)
        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)
        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)
        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)
        # Non-linearities
        self.relu = torch.nn.ReLU()

    def forward(self, X):
        y = self.relu(self.in1(self.conv1(X)))
        y = self.relu(self.in2(self.conv2(y)))
        y = self.relu(self.in3(self.conv3(y)))
        y = self.res1(y)
        y = self.res2(y)
        y = self.res3(y)
        y = self.res4(y)
        y = self.res5(y)
        y = self.relu(self.in4(self.deconv1(y)))
        y = self.relu(self.in5(self.deconv2(y)))
        y = self.deconv3(y)
        return y

def get_relevant_indices(dataset):
  train_indices = list(range(int(0.3*len(dataset))))
  val_indices = list(range(int(0.7*len(dataset)), int(0.85*len(dataset))))
  test_indices = list(range(int(0.85*len(dataset)), len(dataset)+1))
  return train_indices, val_indices, test_indices

"""# Training Code
In this section, hyperparameters for the discriminator and the training network are defined. Furthermore, the forward and backward passes for the neural network training loop are also executed here.
"""

def train(transform_model, content_set, train_indices, val_indices, style_set, batch_size = 1, learning_rate = 0.005, num_epochs=1000, save_name = 'model_graphite'):
  iteration = 20000
  betas = {'conv1_1': 1.,
           'conv2_1': 0.8,
           'conv3_1': 0.3,
           'conv4_1': 0.25,
           'conv5_1': 0.2}

  alpha = 1
  beta = 1500000
  
  np.random.seed(1000)

  train_sampler = SubsetRandomSampler(train_indices)
  content_loader = torch.utils.data.DataLoader(content_set, batch_size=batch_size,
                                               num_workers=1, sampler=train_sampler)
  
  val_sampler = SubsetRandomSampler(val_indices)
  content_val_loader = torch.utils.data.DataLoader(content_set, batch_size=batch_size,
                                               num_workers=1, sampler=val_sampler)
  
  style_set = nc.SafeDataset(style_set)
  style_loader = nc.SafeDataLoader(style_set, batch_size = batch_size, shuffle = True, num_workers = 1) 

  optimizer = optim.Adam(transform_model.parameters(), lr=learning_rate)
  transform_model.train()
  train_loss = []
  for i in range(num_epochs):
    for j, (content_img,__) in enumerate(content_loader):
      content_batch = content_img
      target_batch = content_batch.clone().requires_grad_(True)
      content_rep = get_content_rep(content_batch)
      for style_steps, (style_imgs,__) in enumerate(style_loader):
        transformed_img = transform_model(target_batch.cuda())
        target_reps = get_target_rep(transformed_img)
        s_loss = 0
        c_loss = get_content_loss(content_rep['conv4_2'],target_reps['conv4_2'])
        style_rep = get_style_rep(style_imgs)
        style_gs = {layer: gram_matrix(style_rep[layer]) for layer in style_rep}
        for layer in betas:
          target_rep = target_reps[layer]
          target_g = gram_matrix(target_rep)
          style_g  = style_gs[layer]
          s_loss_temp = betas[layer]*get_style_loss(style_g, target_g)
          s_loss += s_loss_temp
          
        loss = alpha * c_loss + beta * s_loss
        loss.backward(retain_graph=True)
        optimizer.step()
        optimizer.zero_grad()
      if j % 50 == 0:
        print('EPOCH:{} current loss:{} '.format(i, loss.item()))
        torch.save(transform_model.state_dict(), '/content/gdrive/My Drive/APS360_Style_Transfer/Saved Models/'+ save_name)
        img = content_img.to("cpu").detach()
        img = img[0].numpy()
        img = img.transpose(1,2,0)
        img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))
        img = img.clip(0, 1)
        plt.imshow(img)
        plt.show()
        img = transformed_img.to("cpu").detach()
        img = img[0].numpy()
        img = img.transpose(1,2,0)
        img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))
        img = img.clip(0, 1)
        plt.imshow(img)
        plt.show()
      
      if j % 100 == 0:
        train_loss.append(loss.item())

  n = len(train_loss)
  plt.title("Train Loss")
  plt.plot(range(1,n+1), train_loss, label="Train")
  plt.xlabel("Interation")
  plt.ylabel("Loss")
  plt.legend(loc='best')
  plt.show()

"""# Training the actual model to stylize images based on pencil sketches, oil paintings and watercolour paintings"""

train_indices, val_indices, test_indices = get_relevant_indices(content_set)
transformer = TransformerNet().cuda()
transformer.load_state_dict(torch.load('/content/gdrive/My Drive/APS360_Style_Transfer/Saved Models/model_graphite'))
train(transform_model = transformer, content_set = content_set, train_indices = train_indices, val_indices = val_indices, style_set = graphite_set, batch_size = 1, learning_rate = 0.005, num_epochs=10, save_name = 'model_graphite2')

train_indices, val_indices, test_indices = get_relevant_indices(content_set)
transformer = TransformerNet().cuda()
transformer.load_state_dict(torch.load('/content/gdrive/My Drive/APS360_Style_Transfer/Saved Models/model_oilpaint'))
train(transform_model = transformer, content_set = content_set, train_indices = train_indices, val_indices = val_indices, style_set = oilpaint_set, batch_size = 2, learning_rate = 0.0005, num_epochs=5, save_name = 'model_oilpaint')

transformer = TransformerNet().cuda()
transformer.load_state_dict(torch.load('/content/gdrive/My Drive/APS360_Style_Transfer/Saved Models/model_watercolour'))
train(transform_model = transformer, content_set = content_set, train_indices = train_indices, val_indices = val_indices, style_set = watercolour_set, batch_size = 1, learning_rate = 0.005, num_epochs=10, save_name = 'model_watercolour')

"""# Producing a stylized Image based on previous training"""

transformer = TransformerNet().cuda()
transformer.load_state_dict(torch.load('/content/gdrive/My Drive/APS360_Style_Transfer/Saved Models/model_watercolour'))
print(content_set[0][0].shape)
out = transformer(content_set[0][0].unsqueeze(0).cuda())
img = out.to("cpu").detach()
img = img[0].numpy()
img = img.transpose(1,2,0)
img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))
img = img.clip(0, 1)
plt.imshow(img)
plt.show()

img = content_set[0][0].unsqueeze(0).to("cpu").detach()
img = img[0].numpy()
img = img.transpose(1,2,0)
img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))
img = img.clip(0, 1)
plt.imshow(img)
plt.show()

"""# Baseline Model 
As mentioned in the project information document, in this loop the pixels of the images are optimized
"""

def train(content_set, style_set, batch_size = 1, learning_rate = 0.005, num_epochs=1000):
  iteration = 20000
  betas = {'conv1_1': 1.,
           'conv2_1': 0.8,
           'conv3_1': 0.3,
           'conv4_1': 0.25,
           'conv5_1': 0.2}

  alpha = 1
  beta = 1500000
  
  np.random.seed(1000)

  content_img = content_set[1600][0].unsqueeze(0).to("cpu").detach()
  transformed_img = content_img.clone().requires_grad_(True)
  
  style_set = nc.SafeDataset(style_set)
  style_loader = nc.SafeDataLoader(style_set, batch_size = batch_size, shuffle = True, num_workers = 1) 

  optimizer = optim.Adam([transformed_img], lr=learning_rate)
  train_loss = []
  for i in range(num_epochs):
    content_batch = content_img
    content_rep = get_content_rep(content_batch)
    for style_steps, (style_imgs,__) in enumerate(style_loader):
      target_reps = get_target_rep(transformed_img)
      s_loss = 0
      c_loss = get_content_loss(content_rep['conv4_2'],target_reps['conv4_2'])
      style_rep = get_style_rep(style_imgs)
      style_gs = {layer: gram_matrix(style_rep[layer]) for layer in style_rep}
      for layer in betas:
        target_rep = target_reps[layer]
        target_g = gram_matrix(target_rep)
        style_g  = style_gs[layer]
        s_loss_temp = betas[layer]*get_style_loss(style_g, target_g)
        s_loss += s_loss_temp
        
      loss = alpha * c_loss + beta * s_loss
      loss.backward(retain_graph=True)
      optimizer.step()
      optimizer.zero_grad()
    if i % 50 == 0:
      print('EPOCH:{} current loss:{} '.format(i, loss.item()))
      img = content_img.to("cpu").detach()
      img = img[0].numpy()
      img = img.transpose(1,2,0)
      img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))
      img = img.clip(0, 1)
      plt.imshow(img)
      plt.show()
      img = transformed_img.to("cpu").detach()
      img = img[0].numpy()
      img = img.transpose(1,2,0)
      img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))
      img = img.clip(0, 1)
      plt.imshow(img)
      plt.show()
    
    if i % 100 == 0:
      train_loss.append(loss.item())

  n = len(train_loss)
  plt.title("Train Loss")
  plt.plot(range(1,n+1), train_loss, label="Train")
  plt.xlabel("Interation")
  plt.ylabel("Loss")
  plt.legend(loc='best')
  plt.show()

"""# Baseline Image Optimization Iterations"""

train(content_set = content_set, style_set = watercolour_set, batch_size = 1, learning_rate = 0.005, num_epochs=10000)